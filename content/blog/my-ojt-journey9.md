---
title: "My OJT in NCIP"
date: "2026-02-02"
dateEnd: "2026-02-06"
excerpt: "WEEK 9"
---

![4.PNG](/blog-images/1772067351916-4.PNG)
![1.PNG](/blog-images/1772066612587-1.PNG)
![2.PNG](/blog-images/1772066616950-2.PNG)

## February 2, 2026

*First Steps into Actual Coding*

Today, I officially began the initial setup of the Face Recognition system. I started by preparing the React environment and making sure the camera could be accessed properly through the browser.

Seeing the live camera feed appear on the screen felt like real progress. Even though it was just the basic setup, it marked the transition from pure planning to actual implementation.

## February 3, 2026

*Implementing Face Detection*

Today, I worked on integrating the face detection model using TensorFlow.js. After loading the model and connecting it to the camera feed, I focused on detecting faces in real time.

When the first face bounding box appeared on the screen, it was a rewarding moment. However, I also noticed performance drops, which reminded me that optimization will be an important part of development.

## February 4, 2026

*Improving Detection Accuracy*

Today, I focused on refining how detection updates on each frame. I experimented with limiting the detection rate to maintain smoother performance while still keeping the response accurate.

I realized that balancing speed and accuracy is more challenging than I expected. This helped me better understand how real-time AI systems require careful tuning.

## February 5, 2026


*Adding Emotion Detection Logic*

Today, I began experimenting with emotion detection from facial landmarks. I worked on displaying the dominant detected emotion beside the face bounding box.

Although emotion detection is not perfectly accurate, seeing live emotional changes reflected on the screen made the system feel more intelligent and interactive. This was one of the most exciting milestones so far.

## February 6, 2026

*System Refinement and Testing*

Today was focused on testing the overall system flow â€” camera startup, detection, recognition placeholder logic, and emotion updates. I also reviewed how the UI responds when no face is detected or when permission is denied.

This testing phase made me realize the importance of small UX details. Even minor improvements in feedback messages and visual clarity made the system feel more complete and professional.![3.PNG](/blog-images/1772067347343-3.PNG)

